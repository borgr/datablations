import argparse
import os
import pickle
from bisect import bisect_right
from collections import defaultdict
from multiprocessing import cpu_count

from datasets import load_from_disk
from tqdm import tqdm


def get_pairs(byterange):
    """
    Returns pairs generated by
    https://github.com/google-research/deduplicate-text-datasets#collecting-the-duplicates-together
    """
    print("Getting pairs")
    pairs = []
    with open(byterange, "r") as f:
        save = False
        for line in tqdm(f):
            if line.strip() == "out":
                save = True
                continue
            if save:
                left, right = line.strip().split()
                pairs.append((int(left), int(right)))
    print("num pairs", len(pairs))
    return pairs


def get_bytes(pairs, data, offset=0):
    """
    Return bytes constituring the duplicated substring. There seems to be something off here, see:
    https://github.com/google-research/deduplicate-text-datasets/issues/24
    """
    print("Getting bytes")
    byte_array = []
    for left, right in tqdm(pairs):
        if left + offset >= right - offset:
            raise ValueError
        byte_array.append(data[left + offset : right - offset])

    print("byte_array size", len(byte_array))
    return byte_array


def get_doc_id(pos, pos2id, pos2id_list):
    """
    Gets id of the datapoint at position.
    """
    pos = bisect_right(pos2id_list, pos)
    doc_id = pos2id[pos2id_list[pos - 1]]
    return doc_id


def get_url(row, dataset_name):
    if dataset_name == "oscar":
        return row["meta"]["warc_headers"]["warc-target-uri"]
    if dataset_name == "the_pile" or dataset_name == "roots_en":
        return None
    return None


def get_cluster(row, repetitions):
    return [set(repetitions[rep]) for rep in row["repetitions"]]


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--dataset_name",
        "-d",
        type=str,
        required=True,
    )
    args = parser.parse_args()
    dataset_name = args.dataset_name

    HUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN")

    print("Loading", dataset_name)
    dataset = load_from_disk("/home/piktus_huggingface_co/lumi/preprocessed_data/{}".format(dataset_name))["train"]
    print("Done loading", dataset_name)

    base_dir = "/mnt/disks/looking_glass_storage/lumi/dedup/"
    data_path = base_dir + "/{ds}/{ds}.train".format(ds=dataset_name)
    byterange = base_dir + "/{ds}/{ds}.train.byterange".format(ds=dataset_name)
    pairs = get_pairs(byterange)
    data = open(data_path, "rb").read()
    byte_array = get_bytes(pairs, data)
    pos2id = pickle.load(open(base_dir + "/{ds}/{ds}.train.pos2id.pkl".format(ds=dataset_name), "rb"))
    pos2id_list = sorted(pos2id.keys())

    doc_pairs = defaultdict(list)
    doc_bytes = defaultdict(list)
    dup_ratios = defaultdict(float)
    repetitions = defaultdict(list)

    print("Calculating repetitions")
    for (l, r), b in tqdm(zip(pairs, byte_array)):
        i = get_doc_id(l, pos2id, pos2id_list)
        doc_pairs[i].append((l, r))
        doc_bytes[i].append(b)
        repetitions[b].append(i)

    print("Calculating ratios")
    for i, pairs in tqdm(doc_pairs.items()):
        dup_len = sum([r - l for l, r in pairs])
        dup_ratios[i] = dup_len / len(dataset[i]["text"])

    def add_duplication_info(example, idx):
        example["url"] = get_url(example, dataset_name)
        example["domain"] = example["url"].split("/")[2] if example["url"] is not None else None
        example["perplexity"] = example["meta"]["perplexity_score"]
        example["dup_ratio"] = dup_ratios[idx]
        example["pairs"] = doc_pairs[idx]
        example["repetitions"] = doc_bytes[idx]
        example["cluster"] = get_cluster(example, repetitions)

        return example

    dataset = dataset.map(add_duplication_info, num_proc=cpu_count(), with_indices=True, new_fingerprint="13")
    dataset.remove_columns("meta")
    dataset.push_to_hub(
        "datablations/{}".format(dataset_name),
        private=True,
        token=os.environ.get("HUGGINGFACE_TOKEN"),
    )
